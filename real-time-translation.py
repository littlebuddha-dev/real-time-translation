# path: ./real-time-translation.py
# title: è‡ªå‹•ç¿»è¨³ç¿»è¨³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import os
# OpenMPã®é‡è¤‡åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
# OpenMPã¨MKLã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æ˜ç¤ºçš„ã«è¨­å®šã—ã¦ç«¶åˆã‚’å›é¿
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["CT2_OMP_NUM_THREADS"] = "1"

import asyncio
import time
import argparse
import json
import sys
import multiprocessing
from collections import deque
from multiprocessing import shared_memory
import uuid

import numpy as np
import sounddevice as sd
import webrtcvad
import aiohttp
import psutil
import requests

# --- åŸºæœ¬è¨­å®š ---
TARGET_SAMPLE_RATE = 16000
WHISPER_MODEL_NAME = "small"
VAD_AGGRESSIVENESS = 2
VAD_FRAME_MS = 30
VAD_NUM_PADDING_FRAMES = 10
MIN_SPEECH_DURATION_S = 0.25
SILENCE_TIMEOUT_S = 0.5
MAX_SPEECH_DURATION_S = 10.0

# --- Ollama è¨­å®š ---
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3n:e4b")
OLLAMA_CHAT_MODEL = os.getenv("OLLAMA_CHAT_MODEL", OLLAMA_MODEL)
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
# ãƒ¢ãƒ‡ãƒ«ã¸ã®æŒ‡ç¤ºã‚’ã‚ˆã‚Šå³æ ¼ã«ã—ã€ä½™è¨ˆãªæƒ…å ±ï¼ˆãƒ­ãƒ¼ãƒå­—ã€ä»£æ›¿æ¡ˆãªã©ï¼‰ã®å‡ºåŠ›ã‚’æŠ‘åˆ¶ã™ã‚‹
OLLAMA_SYSTEM_PROMPT = "Translate the following English text to Japanese. Output ONLY the translated Japanese text. Do not include romaji, explanations, or alternatives."
OLLAMA_CHAT_SYSTEM_PROMPT = "Translate the following text. If the input is Japanese, translate to English. If English, translate to Japanese. Output ONLY the translated text. Do not include romaji, explanations, or alternatives."
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
OLLAMA_WARMUP_TIMEOUT_S = 120

# --- å…±æœ‰ãƒ¡ãƒ¢ãƒªè¨­å®š ---
NUM_SHM_BLOCKS = 10
SHM_BLOCK_SIZE = int(MAX_SPEECH_DURATION_S * TARGET_SAMPLE_RATE * 4 * 1.1)

# -----------------------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -----------------------------------------------------------------------------

def print_startup_info(args: argparse.Namespace):
    """èµ·å‹•æ™‚ã«è¨­å®šæƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹"""
    print("==========================================================")
    print("      ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ  ğŸš€")
    print("==========================================================")
    print("\n[ åˆæœŸè¨­å®š ]")
    print("----------------------------------------------------------")
    print(f"  - Whisperãƒ¢ãƒ‡ãƒ«         : {args.model}")
    print(f"  - VADã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒã‚¹   : {args.vad_agg}")
    print(f"  - ç„¡éŸ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ      : {args.silence_timeout}ç§’")
    print(f"  - æœ€å°ç™ºè©±æ™‚é–“        : {args.min_speech_duration}ç§’")
    print(f"  - æœ€å¤§ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ™‚é–“  : {MAX_SPEECH_DURATION_S}ç§’")
    print(f"  - Ollama URL            : {OLLAMA_BASE_URL}")
    print(f"  - éŸ³å£°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«        : {args.ollama_model}")
    print(f"  - ãƒãƒ£ãƒƒãƒˆç¿»è¨³ãƒ¢ãƒ‡ãƒ«      : {args.ollama_chat_model}")
    print(f"  - OMP_NUM_THREADS       : {os.environ.get('OMP_NUM_THREADS')}")
    print(f"  - MKL_NUM_THREADS       : {os.environ.get('MKL_NUM_THREADS')}")
    print(f"  - CT2_OMP_NUM_THREADS   : {os.environ.get('CT2_OMP_NUM_THREADS')}")
    print("----------------------------------------------------------\n")

def determine_optimal_threads() -> int:
    """ä½¿ç”¨å¯èƒ½ãªCPUã‚³ã‚¢æ•°ã¨ç©ºããƒ¡ãƒ¢ãƒªã«åŸºã¥ã„ã¦ã€Whisperç”¨ã®æœ€é©ãªã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æ±ºå®šã™ã‚‹"""
    total_cores = os.cpu_count() or 1
    mem = psutil.virtual_memory()
    available_gb = mem.available / (1024 ** 3)
    if "OMP_NUM_THREADS" in os.environ:
        try:
            return int(os.environ["OMP_NUM_THREADS"])
        except ValueError:
            pass
    recommended_threads = max(1, total_cores // 2)
    if available_gb > 16: return min(recommended_threads, 8)
    if available_gb > 8: return min(recommended_threads, 4)
    if available_gb > 4: return min(recommended_threads, 2)
    return 1

# -----------------------------------------------------------------------------
# Whisperæ–‡å­—èµ·ã“ã—å°‚ç”¨ãƒ—ãƒ­ã‚»ã‚¹
# -----------------------------------------------------------------------------

def run_transcriber_process(
    speech_queue: multiprocessing.Queue,
    text_queue: multiprocessing.Queue,
    free_shm_queue: multiprocessing.Queue,
    model_name: str
):
    """å…±æœ‰ãƒ¡ãƒ¢ãƒªçµŒç”±ã§éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€faster-whisperã§æ–‡å­—èµ·ã“ã—ã‚’è¡Œã†ãƒ—ãƒ­ã‚»ã‚¹"""
    try:
        import torch
        from faster_whisper import WhisperModel
        import numpy as np

        print("ğŸ§  Whisperãƒ—ãƒ­ã‚»ã‚¹: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        compute_type = "float16" if device == "cuda" else "int8"
        num_threads = determine_optimal_threads()
        model = WhisperModel(model_name, device=device, compute_type=compute_type, cpu_threads=num_threads)
        print(f"âœ… Whisperãƒ—ãƒ­ã‚»ã‚¹: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (Device: {device.upper()}, Type: {compute_type}, Threads: {num_threads})")
        text_queue.put("WHISPER_READY")

    except Exception as e:
        error_message = f"FATAL:whisper_process:ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {type(e).__name__}: {e}"
        print(error_message, file=sys.stderr)
        text_queue.put(error_message)
        return

    while True:
        try:
            shm_info = speech_queue.get()
            if shm_info is None:
                print("ğŸ§  Whisperãƒ—ãƒ­ã‚»ã‚¹: çµ‚äº†ã‚·ã‚°ãƒŠãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚")
                break

            shm_name, data_size, data_dtype_str = shm_info
            data_dtype = np.dtype(data_dtype_str)
            shm = None
            try:
                shm = shared_memory.SharedMemory(name=shm_name)
                audio_segment = np.copy(np.ndarray((data_size,), dtype=data_dtype, buffer=shm.buf))
                
                segments, _ = model.transcribe(
                    audio_segment,
                    language='en',
                    beam_size=5,
                    patience=1.0,
                    log_prob_threshold=-1.0,
                    no_speech_threshold=0.7,
                    compression_ratio_threshold=2.4,
                    temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
                )
                transcript = " ".join([seg.text for seg in segments]).strip()

                if transcript:
                    text_queue.put(f"[WHISPER] {transcript}")
                else:
                    text_queue.put("[WHISPER] (empty)")
            finally:
                if shm:
                    shm.close()
                free_shm_queue.put(shm_name)
        except Exception as e:
            text_queue.put(f"[FATAL] Whisperãƒ—ãƒ­ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
            if 'shm_name' in locals():
                free_shm_queue.put(shm_name)
            break
    print("âœ… Whisperãƒ—ãƒ­ã‚»ã‚¹: æ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")

# -----------------------------------------------------------------------------
# éŸ³å£°å‡¦ç†ã‚¯ãƒ©ã‚¹
# -----------------------------------------------------------------------------

class AudioRecorder:
    """ãƒã‚¤ã‚¯ã‹ã‚‰ã®éŸ³å£°å…¥åŠ›ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, audio_chunk_queue: asyncio.Queue, samplerate: int, frame_duration_ms: int, loop: asyncio.AbstractEventLoop):
        self.audio_chunk_queue = audio_chunk_queue
        self.samplerate = samplerate
        self.frame_duration_ms = frame_duration_ms
        self.chunk_size = int(samplerate * frame_duration_ms / 1000)
        self.loop = loop
        self.stream = None
        print(f"ğŸ™ï¸ AudioRecorder: ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ={self.samplerate}, ãƒ•ãƒ¬ãƒ¼ãƒ æœŸé–“={self.frame_duration_ms}ms, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={self.chunk_size}")

    def _callback(self, indata: np.ndarray, frames: int, time_info, status: sd.CallbackFlags):
        if status:
            self.loop.call_soon_threadsafe(self.audio_chunk_queue.put_nowait, (None, f"\nâš ï¸ ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å•é¡ŒãŒç™ºç”Ÿ: {status}"))
        if self.loop.is_running():
            try:
                self.audio_chunk_queue.put_nowait(('audio', indata[:, 0].copy()))
            except asyncio.QueueFull:
                pass

    def start(self):
        try:
            sd.check_input_settings()
            self.stream = sd.InputStream(samplerate=self.samplerate, blocksize=self.chunk_size, channels=1, dtype='float32', callback=self._callback)
            self.stream.start()
            print("âœ… AudioRecorder: ãƒã‚¤ã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒã‚¤ã‚¯ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒã‚¤ã‚¹ãŒæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚: {e}", file=sys.stderr)
            raise

    def stop(self):
        if self.stream:
            self.stream.stop()
            self.stream.close()
            self.stream = None
            print("âœ… AudioRecorder: ãƒã‚¤ã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")

# -----------------------------------------------------------------------------
# éåŒæœŸã‚¿ã‚¹ã‚¯
# -----------------------------------------------------------------------------

async def vad_processor(
    audio_chunk_queue: asyncio.Queue,
    speech_ipc_queue: multiprocessing.Queue,
    free_shm_queue: multiprocessing.Queue,
    shm_instances: dict,
    print_queue: asyncio.Queue,
    loop: asyncio.AbstractEventLoop,
    args: argparse.Namespace
):
    """VADã‚’è¡Œã„ã€ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å…±æœ‰ãƒ¡ãƒ¢ãƒªçµŒç”±ã§é€ä¿¡ã™ã‚‹"""
    vad = webrtcvad.Vad(args.vad_agg)
    silence_timeout_frames = int(args.silence_timeout * 1000 / VAD_FRAME_MS)
    max_speech_duration_frames = int(MAX_SPEECH_DURATION_S * 1000 / VAD_FRAME_MS)
    
    state = "IDLE"
    history_audio_chunks = deque(maxlen=VAD_NUM_PADDING_FRAMES)
    current_speech_chunks = []
    frames_of_silence_after_speech = 0
    
    await print_queue.put(f"ğŸ¯ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: VADã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒã‚¹={args.vad_agg}, ç„¡éŸ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ={silence_timeout_frames}")
    
    last_audio_time = time.monotonic()

    async def send_segment_via_shm(speech_chunks, reason):
        segment_duration_s = (len(speech_chunks) * VAD_FRAME_MS) / 1000.0
        if segment_duration_s < args.min_speech_duration:
            await print_queue.put(f"ğŸ—‘ï¸ VAD: çŸ­ã™ãã‚‹ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç ´æ£„ (é•·ã•: {segment_duration_s:.2f}ç§’)ã€‚")
            return

        shm_name = None
        try:
            shm_name = await loop.run_in_executor(None, free_shm_queue.get)
            speech_segment_np = np.concatenate(speech_chunks).astype(np.float32)
            
            shm = shm_instances[shm_name]
            
            if speech_segment_np.nbytes > shm.size:
                raise MemoryError(f"éŸ³å£°ãƒ‡ãƒ¼ã‚¿({speech_segment_np.nbytes} bytes)ãŒå…±æœ‰ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯({shm.size} bytes)ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚")
            
            shm.buf[:speech_segment_np.nbytes] = speech_segment_np.tobytes()
            
            shm_info = (shm_name, speech_segment_np.size, str(speech_segment_np.dtype))
            await loop.run_in_executor(None, speech_ipc_queue.put, shm_info)
            await print_queue.put(f"ğŸš€ VAD: ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ ({segment_duration_s:.2f}ç§’, {reason}) ã‚’å…±æœ‰ãƒ¡ãƒ¢ãƒª({shm_name})çµŒç”±ã§é€ä¿¡ã€‚")
        except Exception as e:
            await print_queue.put(f"\nâŒ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: å…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ã®æ›¸ãè¾¼ã¿/é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            if shm_name:
                await loop.run_in_executor(None, free_shm_queue.put, shm_name)

    while True:
        try:
            try:
                msg_type, data = await asyncio.wait_for(audio_chunk_queue.get(), timeout=5.0)
                if msg_type is None:
                    await print_queue.put(data)
                    continue

                audio_chunk_float32 = data
                last_audio_time = time.monotonic()
                max_vol = np.max(np.abs(audio_chunk_float32))
                if max_vol > 0.005:
                    await print_queue.put(('level', max_vol))
            except asyncio.TimeoutError:
                if time.monotonic() - last_audio_time > 5.0:
                    await print_queue.put("\n\nâš ï¸ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: 5ç§’ä»¥ä¸ŠéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã—ã¦ã„ã¾ã›ã‚“ã€‚ãƒã‚¤ã‚¯ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ã‹ã€ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã§ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n")
                    last_audio_time = time.monotonic()
                continue

            audio_chunk_int16 = (audio_chunk_float32 * 32767).astype(np.int16)
            
            if audio_chunk_int16.nbytes != (TARGET_SAMPLE_RATE // 1000 * VAD_FRAME_MS * 2):
                history_audio_chunks.append(audio_chunk_float32)
                audio_chunk_queue.task_done()
                continue
            
            is_speech_now = vad.is_speech(audio_chunk_int16.tobytes(), TARGET_SAMPLE_RATE)
            
            if state == "IDLE":
                if is_speech_now:
                    await print_queue.put("\n")
                    state = "SPEAKING"
                    current_speech_chunks = list(history_audio_chunks) + [audio_chunk_float32]
                    await print_queue.put("ğŸ—£ï¸ VAD: IDLE -> SPEAKING (ç™ºè©±é–‹å§‹)")
                else:
                    history_audio_chunks.append(audio_chunk_float32)
            elif state == "SPEAKING":
                current_speech_chunks.append(audio_chunk_float32)
                if not is_speech_now:
                    state = "PENDING_END"
                    frames_of_silence_after_speech = 1
                elif len(current_speech_chunks) >= max_speech_duration_frames:
                    await send_segment_via_shm(current_speech_chunks, "å¼·åˆ¶çµ‚äº†")
                    state = "IDLE"
                    current_speech_chunks.clear()
                    history_audio_chunks.clear()
                    await print_queue.put("ğŸ”„ VAD: SPEAKING -> IDLE (å¼·åˆ¶çµ‚äº†)")
            elif state == "PENDING_END":
                current_speech_chunks.append(audio_chunk_float32)
                if is_speech_now:
                    state = "SPEAKING"
                    frames_of_silence_after_speech = 0
                else:
                    frames_of_silence_after_speech += 1
                    if frames_of_silence_after_speech >= silence_timeout_frames:
                        speech_only_chunks = current_speech_chunks[:-frames_of_silence_after_speech]
                        await send_segment_via_shm(speech_only_chunks, "ç„¡éŸ³æ¤œå‡º")
                        state = "IDLE"
                        current_speech_chunks.clear()
                        history_audio_chunks.clear()
                        await print_queue.put("ğŸ”„ VAD: PENDING_END -> IDLE (ç™ºè©±çµ‚äº†)")
            audio_chunk_queue.task_done()
        except asyncio.CancelledError:
            await print_queue.put("VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            break
        except Exception as e:
            await print_queue.put(f"\nâŒ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")

async def ollama_translator(text_queue: asyncio.Queue, print_queue: asyncio.Queue, session: aiohttp.ClientSession, args: argparse.Namespace):
    """æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’Ollamaã§ç¿»è¨³ã™ã‚‹"""
    await print_queue.put("âœ… OllamaéŸ³å£°ç¿»è¨³ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    while True:
        english_text = await text_queue.get()
        if english_text.startswith("[WHISPER]"):
            content = english_text.replace("[WHISPER]", "").strip()
            if content == "(empty)": continue
            
            await print_queue.put(f"ğŸ—£ï¸ Transcription: {content}")
            payload = { "model": args.ollama_model, "stream": True, "messages": [{"role": "system", "content": args.ollama_prompt}, {"role": "user", "content": content}] }
            await _process_ollama_stream(session, f"{OLLAMA_BASE_URL}/api/chat", payload, "ğŸŒ Translation: ", print_queue)
        text_queue.task_done()

async def chat_input_handler(chat_queue: asyncio.Queue, print_queue: asyncio.Queue, loop: asyncio.AbstractEventLoop):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’éåŒæœŸã§å—ã‘ä»˜ã‘ã‚‹"""
    while True:
        await print_queue.put(('prompt', None))
        message = await loop.run_in_executor(None, sys.stdin.readline)
        if message.strip():
            await chat_queue.put(message.strip())

async def ollama_chat_translator(chat_queue: asyncio.Queue, print_queue: asyncio.Queue, session: aiohttp.ClientSession, args: argparse.Namespace):
    """ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’Ollamaã§ç¿»è¨³ã™ã‚‹"""
    await print_queue.put("âœ… Ollamaãƒãƒ£ãƒƒãƒˆç¿»è¨³ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    while True:
        text_to_translate = await chat_queue.get()
        await print_queue.put(f"ğŸ’¬ You: {text_to_translate}")
        payload = { "model": args.ollama_chat_model, "stream": True, "messages": [{"role": "system", "content": args.ollama_chat_prompt}, {"role": "user", "content": text_to_translate}] }
        await _process_ollama_stream(session, f"{OLLAMA_BASE_URL}/api/chat", payload, "ğŸ¤– Chat Translation: ", print_queue)
        chat_queue.task_done()

async def _process_ollama_stream(session: aiohttp.ClientSession, url: str, payload: dict, output_prefix: str, print_queue: asyncio.Queue):
    """Ollama APIã¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€šä¿¡ã‚’å‡¦ç†ã™ã‚‹å…±é€šé–¢æ•°"""
    full_translation = ""
    try:
        async with session.post(url, json=payload, headers={"Accept": "application/json"}) as resp:
            if resp.status == 200:
                async for chunk in resp.content:
                    if not chunk: continue
                    try:
                        for line in chunk.decode('utf-8').strip().split('\n'):
                            if not line: continue
                            data = json.loads(line)
                            content_part = data.get("response", "") if "response" in data else data.get("message", {}).get("content", "")
                            full_translation += content_part
                            await print_queue.put(('stream', f"{output_prefix}{full_translation}"))
                            if data.get("done"):
                                await print_queue.put("\n")
                                return
                    except (json.JSONDecodeError, UnicodeDecodeError): continue
            else:
                error_text = await resp.text()
                await print_queue.put(f"\nâŒ Ollama APIã‚¨ãƒ©ãƒ¼ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {resp.status}): {error_text}")
    except Exception as e:
        await print_queue.put(f"\nâŒ Ollamaé€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

async def ipc_text_queue_reader(ipc_queue: multiprocessing.Queue, asyncio_queue: asyncio.Queue, loop: asyncio.AbstractEventLoop):
    """ãƒ—ãƒ­ã‚»ã‚¹é–“ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿å–ã‚Šã€asyncioã‚­ãƒ¥ãƒ¼ã«æ¸¡ã™ãƒ–ãƒªãƒƒã‚¸"""
    print("Bridge: IPCãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ¥ãƒ¼ãƒªãƒ¼ãƒ€ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    while True:
        try:
            text = await loop.run_in_executor(None, ipc_queue.get)
            if text is None: break
            if isinstance(text, str) and text.startswith("[FATAL]"):
                print(f"âŒ Whisperãƒ—ãƒ­ã‚»ã‚¹ã§è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼:\n   {text}", file=sys.stderr)
                loop.stop()
                break
            await asyncio_queue.put(text)
        except (asyncio.CancelledError, BrokenPipeError): break
        except Exception as e:
            print(f"âŒ Bridge: IPCãƒªãƒ¼ãƒ€ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            break

async def console_printer(print_queue: asyncio.Queue):
    """ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¸ã®å‡ºåŠ›ã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹"""
    while True:
        try:
            item = await print_queue.get()
            if isinstance(item, tuple):
                msg_type, content = item
                if msg_type == 'stream':
                    print(f"\r{content}", end="", flush=True)
                elif msg_type == 'level':
                    bar = 'â–ˆ' * int(content * 300)
                    print(f"\rğŸ¤ Input level: [{bar:<50}] {content:.3f}", end="", flush=True)
                elif msg_type == 'prompt':
                    print("\n> ", end="", flush=True)
            else:
                print(f"\r{item}{' ' * 20}\n", end="", flush=True)

            print_queue.task_done()
        except asyncio.CancelledError:
            break

# -----------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
# -----------------------------------------------------------------------------

async def run_main_loop(args, speech_ipc_queue, text_ipc_queue, free_shm_queue, shm_instances):
    loop = asyncio.get_running_loop()

    audio_chunk_queue = asyncio.Queue(maxsize=300)
    transcribed_text_queue = asyncio.Queue(maxsize=20)
    chat_text_queue = asyncio.Queue(maxsize=10)
    print_queue = asyncio.Queue()

    audio_recorder = AudioRecorder(audio_chunk_queue, TARGET_SAMPLE_RATE, VAD_FRAME_MS, loop)
    try:
        audio_recorder.start()
    except Exception as e:
        await print_queue.put(f"âŒ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—: ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ¬ã‚³ãƒ¼ãƒ€ãƒ¼ã®èµ·å‹•ã«å¤±æ•—: {e}")
        return

    await print_queue.put("==========================================================")
    await print_queue.put("                âœ¨ ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹• âœ¨")
    await print_queue.put("==========================================================")
    await print_queue.put("ğŸ™ï¸  ãƒã‚¤ã‚¯ã‹ã‚‰ã®éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    await print_queue.put("âŒ¨ï¸  ãƒãƒ£ãƒƒãƒˆç¿»è¨³ãŒæœ‰åŠ¹ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦Enterã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    await print_queue.put("ğŸ›‘ Ctrl+Cã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    await print_queue.put("----------------------------------------------------------\n")

    async with aiohttp.ClientSession() as session:
        all_tasks = {
            asyncio.create_task(console_printer(print_queue)),
            asyncio.create_task(vad_processor(audio_chunk_queue, speech_ipc_queue, free_shm_queue, shm_instances, print_queue, loop, args)),
            asyncio.create_task(ollama_translator(transcribed_text_queue, print_queue, session, args)),
            asyncio.create_task(chat_input_handler(chat_text_queue, print_queue, loop)),
            asyncio.create_task(ollama_chat_translator(chat_text_queue, print_queue, session, args)),
            asyncio.create_task(ipc_text_queue_reader(text_ipc_queue, transcribed_text_queue, loop))
        }
        try:
            await asyncio.gather(*all_tasks)
        except asyncio.CancelledError:
            print("\nâœ… ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—: ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
        finally:
            print("\nğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
            audio_recorder.stop()
            for task in all_tasks: task.cancel()
            await asyncio.gather(*all_tasks, return_exceptions=True)
            print("âœ… ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å…¨ã‚¿ã‚¹ã‚¯ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

def warm_up_ollama(model_name: str, prompt: str, chat: bool):
    """Ollamaãƒ¢ãƒ‡ãƒ«ã‚’åŒæœŸçš„ã«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã™ã‚‹"""
    api_url = f"{OLLAMA_BASE_URL}/api/chat"
    print(f"ğŸ”¥ Ollama{'ãƒãƒ£ãƒƒãƒˆ' if chat else 'éŸ³å£°'}ç¿»è¨³ãƒ¢ãƒ‡ãƒ« ({model_name}) ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
    payload = { "model": model_name, "stream": False, "messages": [{"role": "system", "content": prompt}, {"role": "user", "content": "Hello."}], "options": {"num_predict": 1} }
    try:
        response = requests.post(api_url, json=payload, timeout=OLLAMA_WARMUP_TIMEOUT_S)
        response.raise_for_status()
        print(f"âœ… Ollama{'ãƒãƒ£ãƒƒãƒˆ' if chat else 'éŸ³å£°'}ç¿»è¨³ãƒ¢ãƒ‡ãƒ« ({model_name}) ã®æº–å‚™å®Œäº†ã€‚")
        return True
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ollamaã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•— ({model_name}): {e}", file=sys.stderr)
        return False

def main():
    parser = argparse.ArgumentParser(description="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‹±èªéŸ³å£°æ–‡å­—èµ·ã“ã—ï¼†æ—¥æœ¬èªç¿»è¨³")
    parser.add_argument("--model", type=str, default=WHISPER_MODEL_NAME, help=f"Whisperãƒ¢ãƒ‡ãƒ«å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {WHISPER_MODEL_NAME})")
    parser.add_argument("--vad_agg", type=int, default=VAD_AGGRESSIVENESS, choices=[0,1,2,3], help=f"VADã®ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒã‚¹(0-3) (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {VAD_AGGRESSIVENESS})")
    parser.add_argument("--silence_timeout", type=float, default=SILENCE_TIMEOUT_S, help=f"ç„¡éŸ³è¨±å®¹æ™‚é–“(ç§’) (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {SILENCE_TIMEOUT_S})")
    parser.add_argument("--min_speech_duration", type=float, default=MIN_SPEECH_DURATION_S, help=f"æœ€å°ç™ºè©±æ™‚é–“(ç§’) (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {MIN_SPEECH_DURATION_S})")
    parser.add_argument("--ollama_model", type=str, default=OLLAMA_MODEL, help=f"éŸ³å£°ç¿»è¨³ç”¨Ollamaãƒ¢ãƒ‡ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {OLLAMA_MODEL})")
    parser.add_argument("--ollama_prompt", type=str, default=OLLAMA_SYSTEM_PROMPT, help=f"éŸ³å£°ç¿»è¨³ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--ollama_chat_model", type=str, default=OLLAMA_CHAT_MODEL, help=f"ãƒãƒ£ãƒƒãƒˆç¿»è¨³ç”¨Ollamaãƒ¢ãƒ‡ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {OLLAMA_CHAT_MODEL})")
    parser.add_argument("--ollama_chat_prompt", type=str, default=OLLAMA_CHAT_SYSTEM_PROMPT, help=f"ãƒãƒ£ãƒƒãƒˆç¿»è¨³ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    args = parser.parse_args()
    print_startup_info(args)

    shm_instances = {}
    transcriber_process = None
    try:
        for i in range(NUM_SHM_BLOCKS):
            try:
                shm = shared_memory.SharedMemory(create=True, size=SHM_BLOCK_SIZE)
                shm_instances[shm.name] = shm
            except FileExistsError:
                name = list(shm_instances.keys())[-1].name if shm_instances else f"rt_trans_{i}"
                shared_memory.SharedMemory(name=name).unlink()
                shm = shared_memory.SharedMemory(create=True, size=SHM_BLOCK_SIZE)
                shm_instances[shm.name] = shm
        print(f"âœ… {len(shm_instances)}å€‹ã®å…±æœ‰ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯ ({SHM_BLOCK_SIZE/1024/1024:.2f}MB/å€‹) ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
        
        speech_ipc_queue = multiprocessing.Queue()
        text_ipc_queue = multiprocessing.Queue()
        free_shm_queue = multiprocessing.Queue()
        for name in shm_instances.keys():
            free_shm_queue.put(name)

        transcriber_process = multiprocessing.Process(target=run_transcriber_process, args=(speech_ipc_queue, text_ipc_queue, free_shm_queue, args.model), daemon=True)
        transcriber_process.start()
        
        print("â³ Whisperãƒ—ãƒ­ã‚»ã‚¹ã®æº–å‚™ã‚’å¾…æ©Ÿä¸­...")
        try:
            message = text_ipc_queue.get(timeout=OLLAMA_WARMUP_TIMEOUT_S)
            if message != "WHISPER_READY":
                raise RuntimeError(f"Whisperãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã®äºˆæœŸã›ã¬ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}")
            print("âœ… Whisperãƒ—ãƒ­ã‚»ã‚¹æº–å‚™å®Œäº†ã€‚")
        except multiprocessing.queues.Empty:
            raise RuntimeError("Whisperãƒ—ãƒ­ã‚»ã‚¹ã®æº–å‚™å¾…æ©ŸãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚")

        if not warm_up_ollama(args.ollama_model, args.ollama_prompt, chat=False): sys.exit(1)
        if not warm_up_ollama(args.ollama_chat_model, args.ollama_chat_prompt, chat=True): sys.exit(1)

        asyncio.run(run_main_loop(args, speech_ipc_queue, text_ipc_queue, free_shm_queue, shm_instances))

    except KeyboardInterrupt:
        print("\nğŸ›‘ Ctrl+C ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã—ã¾ã™...")
    except Exception as e:
        print(f"\nâŒ è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
    finally:
        print("\nğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        if 'transcriber_process' in locals() and transcriber_process and transcriber_process.is_alive():
            if 'speech_ipc_queue' in locals():
                speech_ipc_queue.put(None)
            transcriber_process.join(timeout=5)
            if transcriber_process.is_alive():
                print("âš ï¸ Whisperãƒ—ãƒ­ã‚»ã‚¹ãŒå¿œç­”ã—ãªã„ãŸã‚ã€å¼·åˆ¶çµ‚äº†ã—ã¾ã™ã€‚")
                transcriber_process.terminate()
                transcriber_process.join()
        
        for shm in shm_instances.values():
            try:
                shm.close()
                shm.unlink()
            except Exception:
                pass
        print(f"âœ… å…±æœ‰ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾ã—ã¾ã—ãŸã€‚")
        print("\nâœ… ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Œå…¨ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    try:
        multiprocessing.set_start_method("spawn")
    except RuntimeError:
        pass
    main()