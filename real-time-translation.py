# path: ./real-time-translation.py
# title: è‡ªå‹•ç¿»è¨³ç¿»è¨³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import os
# OpenMPã®é‡è¤‡åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
# OpenMPã¨MKLã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æ˜ç¤ºçš„ã«è¨­å®šã—ã¦ç«¶åˆã‚’å›é¿
# ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ãƒ¬ãƒãƒ¼ãƒˆã§Thread 13 (OpenMP) ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’1ã«åˆ¶é™ã—ã¦è©¦ã™
os.environ["OMP_NUM_THREADS"] = "1" #
os.environ["MKL_NUM_THREADS"] = "1" #
os.environ["CT2_OMP_NUM_THREADS"] = "1" # ctranslate2 specific

import asyncio
import time
import argparse
import json
import sys
import multiprocessing
from collections import deque

import numpy as np
import sounddevice as sd
import webrtcvad
import aiohttp
import psutil

# --- åŸºæœ¬è¨­å®š ---
TARGET_SAMPLE_RATE = 16000
WHISPER_MODEL_NAME = "base"
VAD_AGGRESSIVENESS = 2
VAD_FRAME_MS = 30
VAD_NUM_PADDING_FRAMES = 10
MIN_SPEECH_DURATION_S = 0.25
SILENCE_TIMEOUT_S = 0.5
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
# VADãŒæ¤œå‡ºã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€å¤§é•·ã‚’åˆ¶é™ã™ã‚‹ (IPCè»¢é€ã‚¨ãƒ©ãƒ¼å¯¾ç­–)
# é•·ã™ãã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒNotImplementedErrorã‚’å¼•ãèµ·ã“ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
MAX_SPEECH_DURATION_S = 10.0 # ä¾‹ãˆã°10ç§’ã«åˆ¶é™
# â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

# --- Ollama è¨­å®š ---
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3n:e4b")
OLLAMA_CHAT_MODEL = os.getenv("OLLAMA_CHAT_MODEL", OLLAMA_MODEL)
OLLAMA_SYSTEM_PROMPT = "You are a helpful AI assistant. Please translate the following English text into short, natural Japanese. Only provide the Japanese translation."
OLLAMA_CHAT_SYSTEM_PROMPT = "You are a versatile AI assistant. Please translate the following text. If the input is Japanese, translate it to English. If the input is English, translate it to Japanese. Provide only the translated text."
OLLAMA_WARMUP_TIMEOUT_S = 30


# -----------------------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# -----------------------------------------------------------------------------

def print_startup_info(args: argparse.Namespace):
    """èµ·å‹•æ™‚ã«è¨­å®šæƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹"""
    print("==========================================================")
    print("      ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ  ğŸš€")
    print("==========================================================")
    print("\n[ åˆæœŸè¨­å®š ]")
    print("----------------------------------------------------------")
    print(f"  - Whisperãƒ¢ãƒ‡ãƒ«         : {args.model}")
    print(f"  - VADã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒã‚¹   : {args.vad_agg}")
    print(f"  - ç„¡éŸ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ      : {args.silence_timeout}ç§’")
    print(f"  - æœ€å°ç™ºè©±æ™‚é–“        : {args.min_speech_duration}ç§’")
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    print(f"  - æœ€å¤§ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ™‚é–“  : {MAX_SPEECH_DURATION_S}ç§’")
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    print(f"  - Ollama URL            : {OLLAMA_BASE_URL}")
    print(f"  - éŸ³å£°ç¿»è¨³ãƒ¢ãƒ‡ãƒ«        : {args.ollama_model}")
    print(f"  - ãƒãƒ£ãƒƒãƒˆç¿»è¨³ãƒ¢ãƒ‡ãƒ«      : {args.ollama_chat_model}")
    print(f"  - OMP_NUM_THREADS       : {os.environ.get('OMP_NUM_THREADS')}")
    print(f"  - MKL_NUM_THREADS       : {os.environ.get('MKL_NUM_THREADS')}")
    print(f"  - CT2_OMP_NUM_THREADS   : {os.environ.get('CT2_OMP_NUM_THREADS')}")
    print("----------------------------------------------------------\n")

def determine_optimal_threads() -> int:
    """
    ä½¿ç”¨å¯èƒ½ãªCPUã‚³ã‚¢æ•°ã¨ç©ºããƒ¡ãƒ¢ãƒªã«åŸºã¥ã„ã¦ã€Whisperç”¨ã®æœ€é©ãªã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æ±ºå®šã™ã‚‹ã€‚
    ãŸã ã—ã€ç’°å¢ƒå¤‰æ•°ã§OMP_NUM_THREADSç­‰ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãã¡ã‚‰ãŒå„ªå…ˆã•ã‚Œã‚‹ãŸã‚ã€
    ã“ã®é–¢æ•°ã¯ä¸»ã«ç›®å®‰ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
    """
    total_cores = os.cpu_count() or 1
    mem = psutil.virtual_memory()
    available_gb = mem.available / (1024 ** 3)

    # ç’°å¢ƒå¤‰æ•°ã§OpenMPã‚¹ãƒ¬ãƒƒãƒ‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆ
    if "OMP_NUM_THREADS" in os.environ:
        try:
            return int(os.environ["OMP_NUM_THREADS"])
        except ValueError:
            pass

    recommended_threads = max(1, total_cores // 2) 

    if available_gb > 16:
        return min(recommended_threads, 8)
    if available_gb > 8:
        return min(recommended_threads, 4)
    if available_gb > 4:
        return min(recommended_threads, 2)
    return 1

# -----------------------------------------------------------------------------
# Whisperæ–‡å­—èµ·ã“ã—å°‚ç”¨ãƒ—ãƒ­ã‚»ã‚¹
# -----------------------------------------------------------------------------

def run_transcriber_process(
    speech_queue: multiprocessing.Queue,
    text_queue: multiprocessing.Queue,
    model_name: str
):
    """
    faster-whisperãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨æ–‡å­—èµ·ã“ã—ã‚’å°‚é–€ã«è¡Œã†ãƒ—ãƒ­ã‚»ã‚¹ã€‚
    ãƒ¡ãƒ¢ãƒªç©ºé–“ã‚’åˆ†é›¢ã™ã‚‹ã“ã¨ã§ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç«¶åˆã‚’é˜²ãã€‚
    """
    try:
        import torch
        from faster_whisper import WhisperModel

        print("ğŸ§  Whisperãƒ—ãƒ­ã‚»ã‚¹: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        compute_type = "float16" if device == "cuda" else "int8"
        
        num_threads = determine_optimal_threads()
        
        model = WhisperModel(
            model_name,
            device=device,
            compute_type=compute_type,
            cpu_threads=num_threads
        )
        
        print(f"âœ… Whisperãƒ—ãƒ­ã‚»ã‚¹: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (Device: {device.upper()}, Type: {compute_type}, Threads: {num_threads})")

    except Exception as e:
        error_message = f"FATAL:whisper_process:ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {type(e).__name__}: {e}"
        print(error_message, file=sys.stderr)
        text_queue.put(error_message)
        return

    while True:
        try:
            audio_segment = speech_queue.get()
            if audio_segment is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                print("ğŸ§  Whisperãƒ—ãƒ­ã‚»ã‚¹: çµ‚äº†ã‚·ã‚°ãƒŠãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚")
                break
            
            segments, _ = model.transcribe(audio_segment, language='en', beam_size=5)
            transcript = " ".join([seg.text for seg in segments]).strip()
            
            if transcript:
                print(f"ğŸ§  Whisperãƒ—ãƒ­ã‚»ã‚¹: æ–‡å­—èµ·ã“ã—çµæœ: \"{transcript}\"")
                text_queue.put(transcript)
            else:
                print("ğŸ§  Whisperãƒ—ãƒ­ã‚»ã‚¹: æ–‡å­—èµ·ã“ã—çµæœãŒç©ºã§ã—ãŸã€‚")
        except Exception as e:
            print(f"âš ï¸ Whisperãƒ—ãƒ­ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}", file=sys.stderr)
            error_message = f"FATAL:whisper_process:æ–‡å­—èµ·ã“ã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}: {e}"
            text_queue.put(error_message)
            break

    print("âœ… Whisperãƒ—ãƒ­ã‚»ã‚¹: æ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")


# -----------------------------------------------------------------------------
# éŸ³å£°å‡¦ç†ã‚¯ãƒ©ã‚¹
# -----------------------------------------------------------------------------

class AudioRecorder:
    """ãƒã‚¤ã‚¯ã‹ã‚‰ã®éŸ³å£°å…¥åŠ›ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, audio_chunk_queue: asyncio.Queue, samplerate: int, frame_duration_ms: int, loop: asyncio.AbstractEventLoop):
        self.audio_chunk_queue = audio_chunk_queue
        self.samplerate = samplerate
        self.frame_duration_ms = frame_duration_ms
        self.chunk_size = int(samplerate * frame_duration_ms / 1000)
        self.loop = loop
        self.stream = None
        print(f"ğŸ™ï¸ AudioRecorder: ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ={self.samplerate}, ãƒ•ãƒ¬ãƒ¼ãƒ æœŸé–“={self.frame_duration_ms}ms, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={self.chunk_size}")

    def _callback(self, indata: np.ndarray, frames: int, time_info, status: sd.CallbackFlags):
        if status:
            print(f"âš ï¸ ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å•é¡ŒãŒç™ºç”Ÿ: {status}", file=sys.stderr)
        
        if self.loop.is_running():
            try:
                qsize = self.audio_chunk_queue.qsize()
                maxsize = self.audio_chunk_queue.maxsize
                if qsize > maxsize * 0.8 and qsize % 10 == 0:
                    print(f"âš ï¸ AudioQueue æ··é›‘: {qsize}/{maxsize}", file=sys.stderr)
                elif qsize == maxsize and (qsize % 10 == 0 or qsize == maxsize):
                    print("âš ï¸ AudioQueue ãŒæº€æ¯ã§ã™ã€‚ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚", file=sys.stderr)

                self.audio_chunk_queue.put_nowait(indata[:, 0].copy())
            except asyncio.QueueFull:
                pass

    def start(self):
        try:
            self.stream = sd.InputStream(
                samplerate=self.samplerate,
                blocksize=self.chunk_size, 
                channels=1,
                dtype='float32', 
                callback=self._callback
            )
            self.stream.start()
            print("âœ… AudioRecorder: ãƒã‚¤ã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒã‚¤ã‚¯ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒã‚¤ã‚¹ãŒæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚: {e}", file=sys.stderr)
            raise

    def stop(self):
        if self.stream:
            self.stream.stop()
            self.stream.close()
            self.stream = None
            print("âœ… AudioRecorder: ãƒã‚¤ã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")

# -----------------------------------------------------------------------------
# éåŒæœŸã‚¿ã‚¹ã‚¯
# -----------------------------------------------------------------------------

async def vad_processor(
    audio_chunk_queue: asyncio.Queue,
    speech_ipc_queue: multiprocessing.Queue,
    loop: asyncio.AbstractEventLoop,
    args: argparse.Namespace
):
    """VADï¼ˆéŸ³å£°åŒºé–“æ¤œå‡ºï¼‰ã‚’è¡Œã„ã€ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åˆ‡ã‚Šå‡ºã™"""
    vad = webrtcvad.Vad(args.vad_agg)
    silence_timeout_frames = int(args.silence_timeout * 1000 / VAD_FRAME_MS)
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    max_speech_duration_frames = int(MAX_SPEECH_DURATION_S * 1000 / VAD_FRAME_MS)
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    
    state = "IDLE"
    history_audio_chunks = deque(maxlen=VAD_NUM_PADDING_FRAMES) 
    current_speech_chunks = []
    frames_of_silence_after_speech = 0
    
    print(f"ğŸ¯ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: VADã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒã‚¹={args.vad_agg}, ç„¡éŸ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ={silence_timeout_frames}")

    vad_process_times = deque(maxlen=100)
    vad_total_chunks_processed = 0

    while True:
        try:
            try:
                audio_chunk_float32 = await asyncio.wait_for(audio_chunk_queue.get(), timeout=VAD_FRAME_MS / 1000.0 * 2)
            except asyncio.TimeoutError:
                continue 
            
            vad_process_start_time = time.monotonic()

            audio_chunk_int16 = (audio_chunk_float32 * 32767).astype(np.int16)

            required_byte_size = (TARGET_SAMPLE_RATE // 1000 * VAD_FRAME_MS * 2)
            
            if audio_chunk_int16.nbytes != required_byte_size:
                history_audio_chunks.append(audio_chunk_float32) 
                audio_chunk_queue.task_done()
                continue

            audio_chunk_bytes = audio_chunk_int16.tobytes()

            try:
                is_speech_now = vad.is_speech(audio_chunk_bytes, TARGET_SAMPLE_RATE)
            except webrtcvad.Error as e:
                print(f"âš ï¸ VADã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
                is_speech_now = False

            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
            # MAX_SPEECH_DURATION_S ã‚’è¶…ãˆãŸã‚‰å¼·åˆ¶çš„ã«ç™ºè©±çµ‚äº†ã¨ã¿ãªã™ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ 
            current_speech_duration_frames = len(current_speech_chunks)

            if state == "IDLE":
                if is_speech_now:
                    state = "SPEAKING"
                    current_speech_chunks = list(history_audio_chunks) + [audio_chunk_float32]
                    print("ğŸ—£ï¸ VAD: IDLE -> SPEAKING (ç™ºè©±é–‹å§‹)")
                else:
                    history_audio_chunks.append(audio_chunk_float32)
            
            elif state == "SPEAKING":
                current_speech_chunks.append(audio_chunk_float32)
                if not is_speech_now:
                    state = "PENDING_END"
                    frames_of_silence_after_speech = 1
                
                # æœ€å¤§ç™ºè©±æ™‚é–“ã‚’è¶…ãˆãŸå ´åˆã€å¼·åˆ¶çš„ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµ‚äº†
                elif current_speech_duration_frames >= max_speech_duration_frames:
                    print(f"âš ï¸ VAD: æœ€å¤§ç™ºè©±æ™‚é–“ ({MAX_SPEECH_DURATION_S:.2f}ç§’) ã‚’è¶…ãˆã¾ã—ãŸã€‚å¼·åˆ¶çš„ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    # å¼·åˆ¶çµ‚äº†æ™‚ã¯ã€ç¾åœ¨ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã¾ã§ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ã™ã‚‹
                    speech_only_chunks = current_speech_chunks
                    segment_duration_s = (len(speech_only_chunks) * VAD_FRAME_MS) / 1000.0
                    
                    if segment_duration_s >= args.min_speech_duration:
                        speech_segment_np = np.concatenate(speech_only_chunks)
                        print(f"ğŸš€ VAD: ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡º (é•·ã•: {segment_duration_s:.2f}ç§’)ã€‚Whisperãƒ—ãƒ­ã‚»ã‚¹ã¸é€ä¿¡ä¸­... (å¼·åˆ¶çµ‚äº†)")
                        try:
                            await loop.run_in_executor(None, speech_ipc_queue.put, speech_segment_np)
                            print(f"ğŸš€ VAD: ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’Whisperã‚­ãƒ¥ãƒ¼ã«putã—ã¾ã—ãŸã€‚ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º: {speech_ipc_queue.qsize()}")
                        except Exception as e:
                            print(f"âŒ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: IPCã‚­ãƒ¥ãƒ¼ã¸ã®putã§ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}", file=sys.stderr)
                            print("âŒ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: IPCã‚­ãƒ¥ãƒ¼ãŒæ©Ÿèƒ½ã—ã¦ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢ã—ã¾ã™ã€‚", file=sys.stderr)
                            loop.stop()
                            return
                    else:
                        print(f"ğŸ—‘ï¸ VAD: çŸ­ã™ãã‚‹ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç ´æ£„ (é•·ã•: {segment_duration_s:.2f}ç§’, æœ€å°: {args.min_speech_duration}ç§’)ã€‚")
                    
                    state = "IDLE"
                    current_speech_chunks.clear()
                    history_audio_chunks.clear()
                    print("ğŸ”„ VAD: SPEAKING -> IDLE (å¼·åˆ¶çµ‚äº†ã€çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ)")
            
            elif state == "PENDING_END":
                current_speech_chunks.append(audio_chunk_float32)
                if is_speech_now:
                    state = "SPEAKING"
                    frames_of_silence_after_speech = 0
                else:
                    frames_of_silence_after_speech += 1
                    if frames_of_silence_after_speech >= silence_timeout_frames:
                        speech_only_chunks = current_speech_chunks[:-frames_of_silence_after_speech]
                        
                        segment_duration_s = (len(speech_only_chunks) * VAD_FRAME_MS) / 1000.0
                        
                        if segment_duration_s >= args.min_speech_duration:
                            speech_segment_np = np.concatenate(speech_only_chunks)
                            print(f"ğŸš€ VAD: ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡º (é•·ã•: {segment_duration_s:.2f}ç§’)ã€‚Whisperãƒ—ãƒ­ã‚»ã‚¹ã¸é€ä¿¡ä¸­...")
                            try:
                                await loop.run_in_executor(None, speech_ipc_queue.put, speech_segment_np)
                                print(f"ğŸš€ VAD: ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’Whisperã‚­ãƒ¥ãƒ¼ã«putã—ã¾ã—ãŸã€‚ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º: {speech_ipc_queue.qsize()}")
                            except Exception as e:
                                print(f"âŒ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: IPCã‚­ãƒ¥ãƒ¼ã¸ã®putã§ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}", file=sys.stderr)
                                print("âŒ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: IPCã‚­ãƒ¥ãƒ¼ãŒæ©Ÿèƒ½ã—ã¦ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢ã—ã¾ã™ã€‚", file=sys.stderr)
                                loop.stop()
                                return
                        else:
                            print(f"ğŸ—‘ï¸ VAD: çŸ­ã™ãã‚‹ç™ºè©±ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç ´æ£„ (é•·ã•: {segment_duration_s:.2f}ç§’, æœ€å°: {args.min_speech_duration}ç§’)ã€‚")
                        
                        state = "IDLE"
                        current_speech_chunks.clear()
                        history_audio_chunks.clear()
                        print("ğŸ”„ VAD: PENDING_END -> IDLE (ç™ºè©±çµ‚äº†ã€çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ)")
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
            
            vad_process_end_time = time.monotonic()
            vad_process_time = vad_process_end_time - vad_process_start_time
            vad_process_times.append(vad_process_time)
            vad_total_chunks_processed += 1

            if vad_total_chunks_processed % 100 == 0:
                avg_vad_time = sum(vad_process_times) / len(vad_process_times) * 1000
                print(f"ğŸ“Š VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: éå»100ãƒãƒ£ãƒ³ã‚¯ã®å¹³å‡å‡¦ç†æ™‚é–“: {avg_vad_time:.2f}ms (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦æ±‚: {VAD_FRAME_MS:.2f}ms)")
                if avg_vad_time > VAD_FRAME_MS * 0.8:
                    print("âš ï¸ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: å¹³å‡å‡¦ç†æ™‚é–“ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¦ä»¶ã«è¿‘ã¥ã„ã¦ã„ã¾ã™ã€‚ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

            audio_chunk_queue.task_done()

        except asyncio.CancelledError:
            print("VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            break
        except Exception as e:
            print(f"âŒ VADãƒ—ãƒ­ã‚»ãƒƒã‚µ: äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", file=sys.stderr)
            audio_chunk_queue.task_done()
            break


async def _process_ollama_stream(session: aiohttp.ClientSession, url: str, payload: dict, output_prefix: str):
    """Ollama APIã¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€šä¿¡ã‚’å‡¦ç†ã™ã‚‹å…±é€šé–¢æ•°"""
    full_translation = ""
    print(output_prefix, end="", flush=True)
    try:
        async with session.post(url, json=payload, headers={"Accept": "application/json"}) as resp:
            if resp.status == 200:
                async for chunk in resp.content:
                    if not chunk: continue
                    try:
                        for line in chunk.decode('utf-8').strip().split('\n'):
                            if not line: continue
                            data = json.loads(line)
                            content_part = ""
                            if "message" in data and "content" in data["message"]:
                                content_part = data["message"]["content"]
                            elif "response" in data: 
                                content_part = data["response"]
                            full_translation += content_part
                            print(f"\r{output_prefix}{full_translation}", end="", flush=True)
                            if "done" in data and data["done"]:
                                print()
                                return
                    except (json.JSONDecodeError, UnicodeDecodeError) as e:
                        continue
            else:
                error_text = await resp.text()
                print(f"\nâŒ Ollama APIã‚¨ãƒ©ãƒ¼ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {resp.status}): {error_text}", file=sys.stderr)
                raise Exception(f"Ollama APIã‚¨ãƒ©ãƒ¼: {error_text}")
    except aiohttp.ClientConnectorError as e:
        print(f"\nâŒ Ollamaã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Ollamaã‚µãƒ¼ãƒãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ã€URL ({url}) ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
    except asyncio.TimeoutError:
        print(f"\nâŒ Ollamaã‹ã‚‰ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ã‚µãƒ¼ãƒã®è² è·ãŒé«˜ã„ã‹ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
    except Exception as e:
        print(f"\nâŒ Ollamaé€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)

async def ollama_translator(text_queue: asyncio.Queue, session: aiohttp.ClientSession, args: argparse.Namespace):
    """æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’Ollamaã§ç¿»è¨³ã™ã‚‹"""
    url = f"{OLLAMA_BASE_URL}/api/chat"
    
    print(f"ğŸ”¥ OllamaéŸ³å£°ç¿»è¨³ãƒ¢ãƒ‡ãƒ« ({args.ollama_model}) ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
    warmup_payload = {
        "model": args.ollama_model,
        "stream": False,
        "messages": [{"role": "system", "content": args.ollama_prompt}, {"role": "user", "content": "Hello."}],
        "options": {"num_predict": 1}
    }
    try:
        timeout = aiohttp.ClientTimeout(total=OLLAMA_WARMUP_TIMEOUT_S)
        async with session.post(url, json=warmup_payload, timeout=timeout) as resp:
            if resp.status == 200:
                resp_json = await resp.json()
                if "message" in resp_json and "content" in resp_json["message"]:
                    pass 
                elif "response" in resp_json:
                    pass
                else:
                    print(f"âš ï¸ Ollamaã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: äºˆæœŸã—ãªã„å¿œç­”å½¢å¼ã€‚å¿œç­”: {resp_json}", file=sys.stderr)
                print(f"âœ… OllamaéŸ³å£°ç¿»è¨³ãƒ¢ãƒ‡ãƒ« ({args.ollama_model}) ã®æº–å‚™å®Œäº†ã€‚")
                print(f"ç¿»è¨³ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            else:
                error_text = await resp.text()
                print(f"âš ï¸ Ollamaã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•— (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {resp.status}): {error_text}", file=sys.stderr)
                print("ç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
    except aiohttp.ClientConnectorError as e:
        print(f"âŒ Ollamaã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ã€‚Ollamaã‚µãƒ¼ãƒãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        print("ç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
    except asyncio.TimeoutError:
        print(f"âŒ Ollamaã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({OLLAMA_WARMUP_TIMEOUT_S}ç§’)ã€‚ã‚µãƒ¼ãƒã®èµ·å‹•ãŒé…ã„ã‹ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã¾ã™ã€‚", file=sys.stderr)
        print("ç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Ollamaã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", file=sys.stderr)
        print("ç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)

    while True:
        english_text = await text_queue.get()
        print(f"ğŸ—£ï¸  Transcription: {english_text}")
        payload = {
            "model": args.ollama_model, "stream": True,
            "messages": [{"role": "system", "content": args.ollama_prompt}, {"role": "user", "content": english_text}]
        }
        await _process_ollama_stream(session, url, payload, "ğŸŒ Translation: ")
        text_queue.task_done()

async def chat_input_handler(chat_queue: asyncio.Queue, loop: asyncio.AbstractEventLoop):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’éåŒæœŸã§å—ã‘ä»˜ã‘ã‚‹"""
    print("\n> ", end="", flush=True) 
    while True:
        message = await loop.run_in_executor(None, sys.stdin.readline)
        if message.strip():
            await chat_queue.put(message.strip())
        print("> ", end="", flush=True)

async def ollama_chat_translator(chat_queue: asyncio.Queue, session: aiohttp.ClientSession, args: argparse.Namespace):
    """ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’Ollamaã§ç¿»è¨³ã™ã‚‹"""
    url = f"{OLLAMA_BASE_URL}/api/chat"
    
    print(f"ğŸ”¥ Ollamaãƒãƒ£ãƒƒãƒˆç¿»è¨³ãƒ¢ãƒ‡ãƒ« ({args.ollama_chat_model}) ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
    chat_warmup_payload = {
        "model": args.ollama_chat_model,
        "stream": False,
        "messages": [{"role": "system", "content": args.ollama_chat_prompt}, {"role": "user", "content": "Hello."}],
        "options": {"num_predict": 1}
    }
    try:
        timeout = aiohttp.ClientTimeout(total=OLLAMA_WARMUP_TIMEOUT_S)
        async with session.post(url, json=chat_warmup_payload, timeout=timeout) as resp:
            if resp.status == 200:
                resp_json = await resp.json()
                if "message" in resp_json and "content" in resp_json["message"]:
                    pass 
                elif "response" in resp_json:
                    pass
                else:
                    print(f"âš ï¸ Ollamaãƒãƒ£ãƒƒãƒˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: äºˆæœŸã—ãªã„å¿œç­”å½¢å¼ã€‚å¿œç­”: {resp_json}", file=sys.stderr)
                print(f"âœ… Ollamaãƒãƒ£ãƒƒãƒˆç¿»è¨³ãƒ¢ãƒ‡ãƒ« ({args.ollama_chat_model}) ã®æº–å‚™å®Œäº†ã€‚")
            else:
                error_text = await resp.text()
                print(f"âš ï¸ Ollamaãƒãƒ£ãƒƒãƒˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•— (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {resp.status}): {error_text}", file=sys.stderr)
                print("ãƒãƒ£ãƒƒãƒˆç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
    except aiohttp.ClientConnectorError as e:
        print(f"âŒ Ollamaãƒãƒ£ãƒƒãƒˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«æ¥ç¶šã‚¨ãƒ©ãƒ¼ã€‚Ollamaã‚µãƒ¼ãƒãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        print("ãƒãƒ£ãƒƒãƒˆç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
    except asyncio.TimeoutError:
        print(f"âŒ Ollamaãƒãƒ£ãƒƒãƒˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({OLLAMA_WARMUP_TIMEOUT_S}ç§’)ã€‚", file=sys.stderr)
        print("ãƒãƒ£ãƒƒãƒˆç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Ollamaãƒãƒ£ãƒƒãƒˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", file=sys.stderr)
        print("ãƒãƒ£ãƒƒãƒˆç¿»è¨³æ©Ÿèƒ½ã¯å‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)

    while True:
        text_to_translate = await chat_queue.get()
        print(f"ğŸ’¬ You: {text_to_translate}")
        payload = {
            "model": args.ollama_chat_model, "stream": True,
            "messages": [{"role": "system", "content": args.ollama_chat_prompt}, {"role": "user", "content": text_to_translate}]
        }
        await _process_ollama_stream(session, url, payload, "ğŸ¤– Chat Translation: ")
        chat_queue.task_done()

async def ipc_text_queue_reader(ipc_queue: multiprocessing.Queue, asyncio_queue: asyncio.Queue, loop: asyncio.AbstractEventLoop):
    """ãƒ—ãƒ­ã‚»ã‚¹é–“ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿å–ã‚Šã€asyncioã‚­ãƒ¥ãƒ¼ã«æ¸¡ã™ãƒ–ãƒªãƒƒã‚¸"""
    print("Bridge: IPCãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ¥ãƒ¼ãƒªãƒ¼ãƒ€ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    while True:
        try:
            try:
                text = await loop.run_in_executor(None, ipc_queue.get, 0.1)
            except multiprocessing.queues.Empty:
                await asyncio.sleep(0.01)
                continue
            
            if text is None: 
                print("Bridge: çµ‚äº†ã‚·ã‚°ãƒŠãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚")
                break
            if isinstance(text, str) and text.startswith("FATAL"):
                print(f"âŒ Whisperãƒ—ãƒ­ã‚»ã‚¹ã§è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’åœæ­¢ã—ã¾ã™:\n   {text}", file=sys.stderr)
                loop.stop()
                break
            print(f"Bridge: IPCã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ä¿¡: \"{text}\" -> Asyncioã‚­ãƒ¥ãƒ¼ã¸è»¢é€")
            await asyncio_queue.put(text)
        except (asyncio.CancelledError, BrokenPipeError):
            print("Bridge: ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¾ãŸã¯ãƒ‘ã‚¤ãƒ—ç ´æã«ã‚ˆã‚Šçµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"âŒ Bridge: IPCãƒªãƒ¼ãƒ€ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            break

# -----------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
# -----------------------------------------------------------------------------

async def run_main_loop(args, speech_ipc_queue, text_ipc_queue):
    loop = asyncio.get_running_loop()

    audio_chunk_queue = asyncio.Queue(maxsize=300)
    transcribed_text_queue = asyncio.Queue(maxsize=20)
    chat_text_queue = asyncio.Queue(maxsize=10)

    audio_recorder = AudioRecorder(audio_chunk_queue, TARGET_SAMPLE_RATE, VAD_FRAME_MS, loop)
    try:
        audio_recorder.start()
    except Exception as e:
        print(f"âŒ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—: ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ¬ã‚³ãƒ¼ãƒ€ãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚: {e}", file=sys.stderr)
        return

    print("==========================================================")
    print("                âœ¨ ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹• âœ¨")
    print("==========================================================")
    print("ğŸ™ï¸  ãƒã‚¤ã‚¯ã‹ã‚‰ã®éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    print("âŒ¨ï¸  ãƒãƒ£ãƒƒãƒˆç¿»è¨³ãŒæœ‰åŠ¹ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦Enterã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    print("ğŸ›‘ Ctrl+Cã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    print("----------------------------------------------------------\n")

    async with aiohttp.ClientSession() as session:
        vad_task = asyncio.create_task(vad_processor(audio_chunk_queue, speech_ipc_queue, loop, args))
        translator_task = asyncio.create_task(ollama_translator(transcribed_text_queue, session, args))
        chat_input_task = asyncio.create_task(chat_input_handler(chat_text_queue, loop))
        chat_translator_task = asyncio.create_task(ollama_chat_translator(chat_text_queue, session, args))
        ipc_reader_task = asyncio.create_task(ipc_text_queue_reader(text_ipc_queue, transcribed_text_queue, loop))

        all_tasks = [vad_task, translator_task, chat_input_task, chat_translator_task, ipc_reader_task]
        
        try:
            await asyncio.gather(*all_tasks)
        except asyncio.CancelledError:
            print("\nâœ… ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—: ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"\nâŒ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—: äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", file=sys.stderr)
        finally:
            print("\nğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
            audio_recorder.stop()
            for task in all_tasks:
                if task and not task.done():
                    task.cancel()
            await asyncio.gather(*all_tasks, return_exceptions=True)
            print("âœ… ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å…¨ã‚¿ã‚¹ã‚¯ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

def main():
    parser = argparse.ArgumentParser(description="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‹±èªéŸ³å£°æ–‡å­—èµ·ã“ã—ï¼†æ—¥æœ¬èªç¿»è¨³")
    parser.add_argument("--model", type=str, default=WHISPER_MODEL_NAME, help=f"Whisperãƒ¢ãƒ‡ãƒ«å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {WHISPER_MODEL_NAME})")
    parser.add_argument("--vad_agg", type=int, default=VAD_AGGRESSIVENESS, choices=[0,1,2,3], help=f"VADã®ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒã‚¹(0-3) (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {VAD_AGGRESSIVENESS})")
    parser.add_argument("--silence_timeout", type=float, default=SILENCE_TIMEOUT_S, help=f"ç„¡éŸ³è¨±å®¹æ™‚é–“(ç§’) (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {SILENCE_TIMEOUT_S})")
    parser.add_argument("--min_speech_duration", type=float, default=MIN_SPEECH_DURATION_S, help=f"æœ€å°ç™ºè©±æ™‚é–“(ç§’) (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {MIN_SPEECH_DURATION_S})")
    parser.add_argument("--ollama_model", type=str, default=OLLAMA_MODEL, help=f"éŸ³å£°ç¿»è¨³ç”¨Ollamaãƒ¢ãƒ‡ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {OLLAMA_MODEL})")
    parser.add_argument("--ollama_prompt", type=str, default=OLLAMA_SYSTEM_PROMPT, help=f"éŸ³å£°ç¿»è¨³ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--ollama_chat_model", type=str, default=OLLAMA_CHAT_MODEL, help=f"ãƒãƒ£ãƒƒãƒˆç¿»è¨³ç”¨Ollamaãƒ¢ãƒ‡ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {OLLAMA_CHAT_MODEL})")
    parser.add_argument("--ollama_chat_prompt", type=str, default=OLLAMA_CHAT_SYSTEM_PROMPT, help=f"ãƒãƒ£ãƒƒãƒˆç¿»è¨³ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    args = parser.parse_args()

    print_startup_info(args)

    speech_ipc_queue = multiprocessing.Queue()
    text_ipc_queue = multiprocessing.Queue()

    transcriber_process = multiprocessing.Process(
        target=run_transcriber_process,
        args=(speech_ipc_queue, text_ipc_queue, args.model),
        daemon=True
    )
    transcriber_process.start()
    
    print("â³ Whisperãƒ—ãƒ­ã‚»ã‚¹ã®èµ·å‹•ã¨ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚’å¾…æ©Ÿä¸­...")
    start_time = time.time()
    
    while transcriber_process.is_alive() and time.time() - start_time < OLLAMA_WARMUP_TIMEOUT_S:
        if not text_ipc_queue.empty():
            try:
                message = text_ipc_queue.get_nowait()
                if isinstance(message, str) and message.startswith("FATAL:whisper_process:"):
                    print(f"âŒ Whisperãƒ—ãƒ­ã‚»ã‚¹ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\n   è©³ç´°: {message}", file=sys.stderr)
                    transcriber_process.join(timeout=1)
                    return
            except multiprocessing.queues.Empty:
                pass 
        time.sleep(0.5)

    if not transcriber_process.is_alive():
        print("âŒ Whisperãƒ—ãƒ­ã‚»ã‚¹ãŒäºˆæœŸã›ãšçµ‚äº†ã—ã¾ã—ãŸã€‚IPCã‚­ãƒ¥ãƒ¼ã¸ã®ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã«å¤±æ•—ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚", file=sys.stderr)
        if text_ipc_queue.empty():
            print("   è©³ç´°: Whisperãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åŸå› ä¸æ˜ã€‚", file=sys.stderr)
        else:
            while not text_ipc_queue.empty():
                try:
                    error_msg = text_ipc_queue.get_nowait()
                    print(f"   è©³ç´°: {error_msg}", file=sys.stderr)
                except multiprocessing.queues.Empty:
                    break
        return

    try:
        asyncio.run(run_main_loop(args, speech_ipc_queue, text_ipc_queue))
    except KeyboardInterrupt:
        print("\nğŸ›‘ Ctrl+C ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã—ã¾ã™...")
    except Exception as e:
        print(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
    finally:
        if 'speech_ipc_queue' in locals():
            print("ãƒ¡ã‚¤ãƒ³: Whisperãƒ—ãƒ­ã‚»ã‚¹ã«çµ‚äº†ã‚·ã‚°ãƒŠãƒ«ã‚’é€ã£ã¦ã„ã¾ã™...")
            try:
                speech_ipc_queue.put(None)
            except Exception as e:
                print(f"âš ï¸ ãƒ¡ã‚¤ãƒ³: speech_ipc_queue.put(None) ã§ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        
        if 'transcriber_process' in locals() and transcriber_process.is_alive():
             print("ğŸ§  Whisperãƒ—ãƒ­ã‚»ã‚¹ã®çµ‚äº†ã‚’å¾…ã£ã¦ã„ã¾ã™...")
             transcriber_process.join(timeout=5)
             if transcriber_process.is_alive():
                print("âš ï¸ Whisperãƒ—ãƒ­ã‚»ã‚¹ãŒå¿œç­”ã—ãªã„ãŸã‚ã€å¼·åˆ¶çµ‚äº†ã—ã¾ã™ã€‚")
                transcriber_process.terminate()
                transcriber_process.join(timeout=2)
        
        print("\nâœ… ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Œå…¨ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    # multiprocessingã®é–‹å§‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
    # macOSã§"spawn"ãŒæ¨å¥¨ã•ã‚Œã‚‹ã‚‚ã®ã®ã€ç‰¹å®šã®ç’°å¢ƒã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã®ç›¸æ€§å•é¡ŒãŒã‚ã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€æ¤œè¨¼ç›®çš„ã§è©¦ã™
    # try:
    #     multiprocessing.set_start_method("spawn")
    # except RuntimeError:
    #     pass
    main()